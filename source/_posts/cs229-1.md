---
title: cs229 lesson 1 Supervised Learning Setup. Linear Regression.
tags:
  - cs229
  - machine learning
categories:
  - machine learning
date: 2018-11-07 21:04:54
---

前言
===========
+ 主要结合李航的《统计学习方法》进行学习
+ 只摘录关键知识点，不做详细笔记，仅为方便日后复习有个方向
+ 学习: 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。 --- Herbert A. Simon

统计学习
===========
+ 统计学习的特点，对象，目的，方法，研究

## 监督学习
+ 由训练资料中学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。
+ 基本概念：input space, output space, feature space
+ 其它名词：instance, feature vector, 联合概率分布
+ 假设空间: $ \mathcal{F} = \\{ f\;|  \mathit{Y} = f(X) \\} $

## 统计学习三要素
+ 方法 = 模型 + 策略 + 算法
### 策略
+ 损失函数
  + 0-1, quadratic, absolute, logarithmic
  + 风险函数(期望损失): $ R_{exp}(f) = E_{p}[L(Y, f(X))] = \int_{x\times y}L(y, f(x))P(x,y)dxdy $
  + 经验风险(经验损失)(empirical loss): $\displaystyle R_{emp}(f) = \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i)) $
  + 根据大数定理, 可用$R_{emp}(f)$估计$R_{exp}(f)$, 但由于现实中样本有限, 甚至很少，所以需要矫正$R_{emp}(f)$
  + 经验风险最小化(ERM)和结构风险最小化(SRM)
    + ERM: 用最优化方法求解$min_{f\in\mathcal{F}}R_{emp}(f)$
    > 样本容量很小时容易过拟合(over-fitting), 但样本容量大时，学习效果很好
    > 当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计(MLE)([证明](http://datahonor.com/2017/03/03/最大似然估计与经验风险最小化/)). 
    + SRM: 等价于正则化(regularizer), 即求 $R_{srm}(f)$
      + 结构风险: $\displaystyle R_{srm}(f) = R_{emp}(f) + \lambda J(f)$ 
    > 其中 $\lambda J(f)$ 位正则化项或罚项(penalty term)
    > $J(f)$是模型空间复杂度, 为定义在$\mathcal{F}$上的泛函. $f$越复杂, $J(f)$越大.
    > $\lambda \ge 0$是系数, 用以权衡经验风险和模型复杂度
    > $R_{srm}(f)$小要求$R_{emp}(f)$和$J(f)$同时小, $R_{srm}(f)$小的模型往往对训练数据以及未知的测试数据都有较好的预测





